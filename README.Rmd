---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
Sys.setenv("HTTP_PROXY"="");Sys.setenv("HTTPS_PROXY"="")
```
# polite <img src="man/figures/logo.png" align="right" />

The goal of `polite` is to promote responsible web etiquette. 

> __"bow and scrape" (verb):__ 
>
> 1) To make a deep bow with the right leg drawn back (thus scraping the floor), left hand pressed across the abdomen, right arm held aside.
>
> 2) _(idiomatic, by extension)_ To behave in a servile, obsequious, or excessively polite manner. [1]                   
>                                             Source: _Wiktionary, The free dictionary_
>

The package's two main functions `bow` and `scrape` define and realize web harvesting session. `bow` is used to introduce the client to the host and ask for permission to scrape (by inquiring against host's robots.txt file), while `scrape` is the main function for retrieving data from the remote server. Once the connection is established, there's no need to `bow` again. Rather, in order to adjust a scraping url the user can simply `nod` to the new path, which updates the session's url making sure that the new location can be clarified against robots.txt

The idea of `polite session` is **seeking permission, taking slowly and never asking twice**.

The package builds on awesome toolkit for defining and managing http session (`httr` and `rvest`), declaring useragent string and investigating site policies (`robotstxt`), while utilizing rate-limiting and reponse caching (`ratelimitr` amd `memoise`).

## Installation

You can install the  development version of `polite` from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("dmi3kno/polite")
```

## Basic Example


This is a basic example which shows you how to retrive list of semi-soft cheeses from www.cheese.com. Here we authenticate a session and then scrape the page with specified parameters. Behind the scenes `polite` inquires downloads `robots.txt`, checks the url and useragent string against it, caches call to robots.txt and to the web page and performs rate limiting.

```{r example}
library(polite)
library(rvest)

session <- bow("https://www.cheese.com/by_type", force = TRUE)
result <- scrape(session, params="t=semi-soft&per_page=100") %>%
  html_nodes("h3") %>% 
  html_text()
head(result)
```

## Extended Example

You can build your own functions that incorporate `bow`, `scrape` (and, if required, `nod`). here we will extend our inquiry into cheeses and will download all cheese names and url's to their information pages. Lets retrieve number of pages per letter in alphabetical list, keeping number of results per page to 100 to minimize number of requests.

```{r}
library(polite)
library(rvest)
library(tidyverse)

session <- bow("https://www.cheese.com/alphabetical")
responses <- map(letters, ~scrape(session, params = paste0("per_page=100&i=",.x)) )
results <- map(responses, ~html_nodes(.x, "#id_page") %>% 
                          html_text() %>% 
                          strsplit("\\s") %>% 
                          unlist() %>%
                          `%||%`(1) %>% 
                          as.numeric() %>% 
                          max(na.rm = TRUE) )
pages_df <- tibble(letter = rep.int(letters, times=unlist(results)),
                   pages = unlist(map(results, ~seq.int(from=1, to=.x))))
pages_df
```

Now that we know how many pages to retrieve from each letter page, lets rotate over letter pages and retrieve cheese names and underlying links to pages. We will need to write a helper function. Our session is still valid and we dont need to `nod` again, because we will not be modifying a page url (note that the field `url` is missing from `scrape` function).

```{r}
get_cheese_page <- function(letter, pages){
 lnks <- scrape(session, params=paste0("per_page=100&i=",letter,"&page=",pages)) %>% 
    html_nodes("h3 a")
 tibble(name=lnks %>% html_text(),
        link=lnks %>% html_attr("href"))
}

df <- pages_df %>% pmap_df(get_cheese_page)
df
```





[1] Wiktionary (2018), The free dictionary, retrieved from https://en.wiktionary.org/wiki/bow_and_scrape
