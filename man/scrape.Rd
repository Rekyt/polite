% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/scrape.R
\name{scrape}
\alias{scrape}
\title{Scrape the content of authorized page/API}
\usage{
scrape(bow, params = NULL, accept = "html", content = NULL,
  verbose = FALSE)
}
\arguments{
\item{bow}{host introduction object of class \code{polite}, \code{session} created by \code{bow()} or \code{nod()}}

\item{params}{character vector of parameters to be appended to url in the format "parameter=value"}

\item{accept}{character value of expected data type to be returned by host (e.g. "html", "json", "xml", "csv", "txt", etc)}

\item{content}{MIME type (aka internet media type) used to override the content type returned by the server.
See http://en.wikipedia.org/wiki/Internet_media_type for a list of common types. You can add \code{charset} parameter to override server's default encoding.}

\item{verbose}{extra feedback from the function. Defaults to FALSE}
}
\value{
Onbject of class \code{httr::response} which can be further processed by functions in \code{rvest} package
}
\description{
Scrape the content of authorized page/API
}
\examples{
\dontrun{
 library(rvest)
 biases <- bow("https://en.wikipedia.org/wiki/List_of_cognitive_biases") \%>\%
   scrape(content="text/html; charset=UTF-8") \%>\%
   html_nodes(".wikitable") \%>\%
   html_table()
 biases
 }


\dontrun{
 library(rvest)
 library(polite)

 host <- "https://www.cheese.com"
 session <- bow(host)

 # scrape pages by re-authenticating on new page and scraping with parameters
 get_cheese <- function(session, path, params){
   nod(session, path) \%>\%
     scrape(params)
 }

 res <- vector("list", 5)
 # iterate over first 5 pages
 for (i in seq(5)){
   res[[i]] <- get_cheese(session,
                path = "alphabetical",
                params = paste0("page=", i)) \%>\%
     html_nodes("h3 a") \%>\%
     html_text()

 }
 res
}

}
